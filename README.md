# üå≥ Botanical Tree Reconstruction

A Python toolkit for 3D tree reconstruction from aerial imagery using COLMAP structure-from-motion, deep learning segmentation, and ray-based point densification.

## üêÜ Quick Start (Recommended)

### 1. Setup
```bash
# Create Python virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Load COLMAP (on HPC systems)
module load colmap/3.8-cuda-11.8-avx512
```

### 2. Run Complete Pipeline
```bash
cd scripts

# Basic usage (full 5-step pipeline)
python run_pipeline.py --images ../data/raw --model ../models/model.pth

# High quality processing
python run_pipeline.py --images ../data/raw --model ../models/model.pth \
    --max-features 50000 --mask-type both --visibility-threshold 0.8
```

### 3. HPC Batch Execution
```bash
# Submit complete pipeline to cluster
bsub < run_pipeline.sh
```

## üìä Pipeline Overview

The complete pipeline consists of 5 automated steps:

1. **üéØ Segmentation** - Generate tree masks using DeepLabV3
2. **üèóÔ∏è COLMAP Reconstruction** - Create initial 3D point cloud from images
3. **üîç Point Filtering** - Remove non-tree points using mask visibility  
4. **‚ú® Ray Enhancement** - Add more points to sparse model via ray casting
5. **üìä Visualization** - Create comprehensive comparison plots

**Input**: Raw aerial images + pre-trained segmentation model  
**Output**: Enhanced sparse 3D point cloud + visualization + processing summary

## üóÇÔ∏è Project Structure

```
BotanicalTreeReconstruction/
‚îú‚îÄ‚îÄ README.md              # This documentation
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .gitignore            # Git ignore patterns
‚îú‚îÄ‚îÄ .gitattributes        # Git LFS configuration
‚îú‚îÄ‚îÄ data/                 # Datasets (raw images, segmentation_mask)
‚îú‚îÄ‚îÄ models/               # Pre-trained segmentation model, Colmap models
‚îú‚îÄ‚îÄ outputs/              # Generated visualizations and summaries
‚îî‚îÄ‚îÄ scripts/              # All processing scripts
    ‚îú‚îÄ‚îÄ run_pipeline.py               # üéØ Complete automation script
    ‚îú‚îÄ‚îÄ run_pipeline.sh               # üöÄ HPC batch script
    ‚îú‚îÄ‚îÄ segmentation_inference.py     # üé≠ Tree segmentation with DeepLabV3
    ‚îú‚îÄ‚îÄ colmap_reconstruction.py       # üèóÔ∏è COLMAP reconstruction
    ‚îú‚îÄ‚îÄ mask_based_filtering.py       # üîç Point cloud filtering
    ‚îú‚îÄ‚îÄ ray_based_enhancement.py      # ‚ú® Ray-based point enhancement
    ‚îú‚îÄ‚îÄ visualize_results.py          # üìä Comprehensive visualization
    ‚îú‚îÄ‚îÄ read_write_model.py           # üõ†Ô∏è COLMAP model I/O utilities
    ‚îî‚îÄ‚îÄ logs/                         # HPC job output logs
```

## üéØ Main Pipeline Script

**`run_pipeline.py`** - Complete automation script that orchestrates all pipeline steps.

### Key Arguments:
- `--images DIR` - Input images directory (required)
- `--model FILE` - Path to segmentation model (.pth file)
- `--mask-type {rough,fine,both}` - Type of masks to generate (default: both)
- `--max-features INT` - Maximum features for COLMAP (default: 20000)  
- `--visibility-threshold FLOAT` - Point filtering threshold 0.0-1.0 (default: 0.5)
- `--combine-masks {or,and}` - How to combine rough/fine masks (default: or)

### Skip Options:
- `--skip-segmentation` - Use existing masks in data/segmentation_masks/
- `--skip-colmap` - Use existing model in models/colmap_reconstruction/
- `--skip-filtering` - Use existing filtered model in models/colmap_filtered/
- `--skip-rays` - Skip ray enhancement (faster, less dense)
- `--skip-visualization` - Skip final visualization

### Advanced Options:
- `--colmap-model DIR` - Path to existing COLMAP model (when skipping COLMAP)
- `--filtered-model DIR` - Path to existing filtered model (when skipping filtering)
- `--continue-on-error` - Continue pipeline even if a step fails
- `--debug` - Enable detailed error output

## üöÄ HPC Batch Script

**`run_pipeline.sh`** - LSF batch script for cluster execution with GPU allocation.

- **Resources**: A100 GPU, 32GB RAM, 6-hour limit
- **Environment**: Loads COLMAP module, uses pip-installed requirements
- **Logs**: Output saved to `logs/JOBID.out` and `logs/JOBID.err`

```bash
# Submit pipeline to cluster
bsub < run_pipeline.sh
```

## üõ†Ô∏è Individual Scripts

For advanced users who need fine-grained control over individual pipeline steps.

### segmentation_inference.py
Generate tree segmentation masks using trained DeepLabV3 model.

**Key Arguments:**
- `--model FILE` - Path to trained segmentation model (.pth)
- `--input DIR` - Input images directory
- `--output DIR` - Output masks directory
- `--mask-type {rough,fine,both}` - Type of masks to generate
- `--device {cpu,cuda,auto}` - Processing device
- `--patch-size INT` - Patch size for fine segmentation (default: 512)

**Usage:**
```bash
# Generate both rough and fine masks
python segmentation_inference.py \
    --model ../models/model.pth \
    --input ../data/raw \
    --output ../data/segmentation_masks \
    --mask-type both --device cuda
```

### colmap_reconstruction.py
Create COLMAP 3D reconstruction from images.

**Key Arguments:**
- `image_dir` - Path to input images directory (required)
- `--output DIR` - Output directory (default: ../models/colmap_output)
- `--max-features INT` - Max features per image (default: 20000)
- `--log-level {DEBUG,INFO,WARNING,ERROR}` - Logging level

**Usage:**
```bash
# Basic reconstruction
python colmap_reconstruction.py ../data/raw

# High quality with 50k features
python colmap_reconstruction.py ../data/raw \
    --output ../models/high_quality_colmap \
    --max-features 30000
```

### mask_based_filtering.py
Filter COLMAP 3D points using segmentation masks with visibility-based thresholding.

**Key Arguments:**
- `--colmap DIR` - COLMAP model directory (required)
- `--images DIR` - Original images directory (required)
- `--rough-mask DIR` - Rough segmentation masks directory (required)
- `--fine-mask DIR` - Fine segmentation masks directory (required)
- `--output DIR` - Output directory for filtered model
- `--visibility-threshold FLOAT` - Keep points visible in ‚â•this fraction of images (default: 0.5)
- `--threshold INT` - Mask pixel threshold value (default: 10)
- `--combine {or,and}` - Mask combination method (default: or)
- `--examples INT` - Number of visualization examples to save (default: 5)

**Usage:**
```bash
# Filter using both rough and fine masks
python mask_based_filtering.py \
    --colmap ../models/colmap_reconstruction/sparse/0 \
    --images ../data/raw \
    --rough-mask ../data/segmentation_masks \
    --fine-mask ../data/segmentation_masks \
    --visibility-threshold 0.5 \
    --combine or
```

### ray_based_enhancement.py
Enhance sparse COLMAP model by adding more points using ray-casting and voxel-based sampling.

**Key Arguments:**
- `--colmap_model_dir DIR` - Input COLMAP model directory (required)
- `--rough_mask_dir DIR` - Directory with rough masks (required)
- `--fine_mask_dir DIR` - Directory with fine masks (required)
- `--output_dir DIR` - Base output directory (required)
- `--output-folder-name NAME` - Specific folder name for output
- `--mask_thresh INT` - Mask threshold for combining rough/fine masks (default: 10)
- `--samples_per_image INT` - Sampled mask pixels per image (default: 1000)
- `--depth_samples INT` - Depth samples per ray (default: 50)
- `--voxel_size FLOAT` - Voxel size in meters (default: 0.02)
- `--min_image_support INT` - Minimum images supporting a voxel (default: 3)

**Usage:**
```bash
# Enhance sparse model using ray casting
python ray_based_enhancement.py \
    --colmap_model_dir ../models/colmap_filtered \
    --rough_mask_dir ../data/segmentation_masks \
    --fine_mask_dir ../data/segmentation_masks \
    --output_dir ../models \
    --output-folder-name colmap_ray_enhanced \
    --samples_per_image 1000
```

### visualize_results.py
Create comprehensive visualizations showing all pipeline stages.

**Key Arguments:**
- `--images DIR` - Original images directory (required)
- `--original_model DIR` - Original COLMAP model directory (required)
- `--filtered_model DIR` - Filtered COLMAP model directory (required)
- `--ray_model DIR` - Ray-enhanced COLMAP model directory
- `--output FILE` - Output visualization image path (required)
- `--masks DIR` - Segmentation masks directory
- `--n_images INT` - Number of sample images to visualize (default: 3)
- `--point-size FLOAT` - Size of projected points (default: 1.0)
- `--mask_type {rough,fine,both}` - Type of masks to display (default: both)
- `--show_combined` - Show additional column with combined filtered + ray points

**Usage:**
```bash
# Create 5-column visualization (default)
python visualize_results.py \
    --images ../data/raw \
    --masks ../data/segmentation_masks \
    --original_model ../models/colmap_reconstruction/sparse/0 \
    --filtered_model ../models/colmap_filtered \
    --ray_model ../models/colmap_ray_enhanced \
    --output ../outputs/pipeline_comparison.png \
    --n_images 3

# Create 6-column visualization with combined view
python visualize_results.py \
    --images ../data/raw \
    --masks ../data/segmentation_masks \
    --original_model ../models/colmap_reconstruction/sparse/0 \
    --filtered_model ../models/colmap_filtered \
    --ray_model ../models/colmap_ray_enhanced \
    --output ../outputs/pipeline_comparison.png \
    --n_images 3 \
    --show_combined
```

Creates a 5-column visualization showing:
1. **Original Image** - Raw input images
2. **Segmentation Mask** - Combined tree segmentation overlaid on images  
3. **Original COLMAP** - Points from initial reconstruction (red)
4. **Filtered COLMAP** - Points after mask-based filtering (blue)
5. **New Ray Points** - Only newly added points from ray enhancement (green)

With `--show_combined`, adds a 6th column:
6. **Combined (Filtered + Ray)** - All points in final enhanced model (purple)


## üìä Output Structure

After running the complete pipeline, your outputs will be organized as:

```
models/
‚îú‚îÄ‚îÄ colmap_reconstruction/    # Step 2 - Initial COLMAP reconstruction
‚îÇ   ‚îú‚îÄ‚îÄ database.db
‚îÇ   ‚îî‚îÄ‚îÄ sparse/0/
‚îÇ       ‚îú‚îÄ‚îÄ cameras.bin
‚îÇ       ‚îú‚îÄ‚îÄ images.bin
‚îÇ       ‚îî‚îÄ‚îÄ points3D.bin
‚îú‚îÄ‚îÄ colmap_filtered/         # Step 3 - Filtered points using masks
‚îÇ   ‚îú‚îÄ‚îÄ cameras.bin
‚îÇ   ‚îú‚îÄ‚îÄ images.bin
‚îÇ   ‚îî‚îÄ‚îÄ points3D.bin
‚îî‚îÄ‚îÄ colmap_ray_enhanced/     # Step 4 - Enhanced sparse model from ray casting
    ‚îú‚îÄ‚îÄ cameras.bin
    ‚îú‚îÄ‚îÄ images.bin
    ‚îî‚îÄ‚îÄ points3D.bin

outputs/
‚îú‚îÄ‚îÄ pipeline_visualization.png    # Step 5 - 5-column comparison plot
‚îú‚îÄ‚îÄ processing_summary.txt       # Detailed processing log
‚îî‚îÄ‚îÄ point_statistics.txt         # Point counts for each stage

data/
‚îî‚îÄ‚îÄ segmentation_masks/          # Step 1 - Generated masks
    ‚îú‚îÄ‚îÄ image1_rough.png
    ‚îú‚îÄ‚îÄ image1_fine.png
    ‚îú‚îÄ‚îÄ image2_rough.png
    ‚îî‚îÄ‚îÄ image2_fine.png
```

## üîß Prerequisites

- **Python 3.7+** with pip
- **COLMAP 3.8+** installed and available in PATH
- **CUDA-capable GPU** (recommended for segmentation and COLMAP)
- **Git LFS** for model files (`git lfs install`)

### Installing COLMAP

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install colmap
```

**From source or HPC systems:**
```bash
# Load as module (HPC)
module load colmap/3.8-cuda-11.8-avx512

# Or build from source - see COLMAP documentation
```

## üèÅ Complete Example Workflow

```bash
# 1. Clone and setup
git clone <your-repo>
cd BotanicalTreeReconstruction
pip install -r requirements.txt

# 2. Add your aerial images
cp /path/to/your/drone/images/* data/raw/

# 3. Run complete pipeline
cd scripts
python run_pipeline.py --images ../data/raw --model ../models/model.pth

# 4. Check results
ls ../models/colmap_ray_enhanced/  # Final enhanced sparse point cloud
ls ../outputs/                     # Visualizations and summaries
```

### For HPC Systems:
```bash
# Submit batch job
bsub < run_pipeline.sh

# Monitor progress
bjobs
tail -f logs/[JOBID].out

# Results in same output structure
```

## üö® Troubleshooting

### Common Issues:

**"COLMAP not found"**
- Ensure COLMAP is installed and in PATH
- On HPC: `module load colmap` before running
- Test with: `colmap --help`

**"CUDA out of memory"** 
- Reduce `--max-features` (try 10000)
- Use `--mask-type rough` only
- Add `--device cpu` for segmentation

**"Model file not found"**
- Ensure model is tracked with Git LFS: `git lfs pull`
- Check file size is >100MB: `ls -lh models/`

**"No images found"**
- Check image directory structure: images should be directly in specified folder
- Supported formats: .jpg, .jpeg, .png
- Ensure absolute paths or run from correct directory

## üìö Citations & References

This pipeline combines several established techniques:

- **COLMAP**: Sch√∂nberger & Frahm. "Structure-from-Motion Revisited." CVPR 2016.
- **DeepLabV3**: Chen et al. "Rethinking Atrous Convolution for Semantic Segmentation." arXiv 2017.
- **Ray Casting**: Custom implementation for point densification in vegetation scenes.

## üìÑ License

[Add your license information here]

---

**üåü Quick Commands Summary:**

```bash
# Complete pipeline (recommended)
python run_pipeline.py --images ../data/raw --model ../models/model.pth

# HPC batch submission  
bsub < run_pipeline.sh

# High quality processing
python run_pipeline.py --images ../data/raw --model ../models/model.pth \
    --max-features 50000 --visibility-threshold 0.8

# Skip time-consuming steps
python run_pipeline.py --images ../data/raw --model ../models/model.pth \
    --skip-segmentation --skip-rays
```

